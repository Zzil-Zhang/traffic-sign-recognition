# train_cnn_paddle.py - å¼ºåˆ¶GPUç‰ˆæœ¬
import os
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as optim
from paddle.optimizer.lr import ReduceOnPlateau, CosineAnnealingDecay
from datetime import datetime
import json
import matplotlib.pyplot as plt
from tqdm import tqdm
import random

from data_utils import create_data_loaders
from cnn_model_paddle import create_model_by_type

class PaddleModelCheckpoint:
    """PaddlePaddleæ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ"""
    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1):
        self.filepath = filepath
        self.monitor = monitor
        self.save_best_only = save_best_only
        self.mode = mode
        self.verbose = verbose
        self.best_value = float('inf') if mode == 'min' else float('-inf')
        
    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return
            
        current = logs.get(self.monitor)
        if current is None:
            return
            
        if self.mode == 'min':
            should_save = current < self.best_value
        else:  # 'max'
            should_save = current > self.best_value
            
        if should_save or not self.save_best_only:
            if self.verbose > 0:
                print(f"ä¿å­˜æ¨¡å‹: {self.filepath} ({self.monitor}: {current:.4f})")
                
            # ä¿å­˜æ¨¡å‹
            paddle.save(self.model.state_dict(), self.filepath)
            
            if should_save:
                self.best_value = current
    
    def set_model(self, model):
        self.model = model

class PaddleEarlyStopping:
    """PaddlePaddleæå‰åœæ­¢å›è°ƒ"""
    def __init__(self, monitor='val_acc', patience=15, min_delta=0.001, restore_best_weights=True, verbose=1):
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.verbose = verbose
        self.wait = 0
        self.stopped_epoch = 0
        self.best_value = float('-inf')
        self.best_weights = None
        
    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return False
            
        current = logs.get(self.monitor)
        if current is None:
            return False
        
        # å¦‚æœæ˜¯å‡†ç¡®ç‡ï¼Œè¶Šå¤§è¶Šå¥½
        should_stop = current - self.best_value < self.min_delta
        
        if not should_stop:
            self.best_value = current
            self.wait = 0
            if self.restore_best_weights:
                self.best_weights = {k: v.clone() for k, v in self.model.state_dict().items()}
        else:
            self.wait += 1
            if self.verbose > 0:
                print(f"æ—©åœè®¡æ•°å™¨: {self.wait}/{self.patience}")
            
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                if self.verbose > 0:
                    print(f"âš ï¸  æå‰åœæ­¢åœ¨ç¬¬ {epoch+1} è½®")
                if self.restore_best_weights and self.best_weights is not None:
                    print("æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡...")
                    self.model.set_state_dict(self.best_weights)
                return True
        return False
    
    def set_model(self, model):
        self.model = model

class CNNTrainerPaddle:
    """CNNè®­ç»ƒå™¨ç±»ï¼ˆå¼ºåˆ¶GPUç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, model_name='traffic_cnn_paddle', image_size=(64, 64)):
        self.model_name = model_name
        self.image_size = image_size
        self.model = None
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¼ºåˆ¶ä½¿ç”¨GPU
        self.force_gpu_setup()
        
        # è®¾ç½®éšæœºç§å­
        self.setup_seeds()
        
        # åˆ›å»ºç›®å½•
        self.create_directories()
    
    def force_gpu_setup(self):
        """å¼ºåˆ¶ä½¿ç”¨GPUï¼Œå¦‚æœå¤±è´¥åˆ™æŠ¥é”™é€€å‡º"""
        print("=" * 60)
        print("ğŸš€ å¼ºåˆ¶GPUæ¨¡å¼å¯åŠ¨")
        print("=" * 60)
        
        try:
            # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ä¾å›¾GPU
            print("å°è¯•ä½¿ç”¨ä¾å›¾GPU (iluvatar_gpu:0)...")
            paddle.set_device('iluvatar_gpu:0')
            device = paddle.device.get_device()
            print(f"âœ… æˆåŠŸä½¿ç”¨ä¾å›¾GPUè®¾å¤‡: {device}")
            
        except Exception as e1:
            print(f"ä¾å›¾GPUè®¾ç½®å¤±è´¥: {e1}")
            
            try:
                # æ–¹æ³•2ï¼šå°è¯•å…¶ä»–GPUè®¾å¤‡åç§°
                print("å°è¯•å…¶ä»–GPUè®¾å¤‡åç§°...")
                for device_name in ['gpu:0', 'gpu', 'cuda:0', 'cuda']:
                    try:
                        paddle.set_device(device_name)
                        device = paddle.device.get_device()
                        print(f"âœ… æˆåŠŸä½¿ç”¨GPUè®¾å¤‡: {device}")
                        break
                    except:
                        continue
                else:
                    raise Exception("æ‰€æœ‰GPUè®¾å¤‡å°è¯•å¤±è´¥")
                    
            except Exception as e2:
                print(f"æ‰€æœ‰GPUå°è¯•å¤±è´¥: {e2}")
                print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUè®¾å¤‡ï¼")
                print("è¯·æ£€æŸ¥ï¼š")
                print("1. æ˜¯å¦åœ¨æ”¯æŒGPUçš„ç¯å¢ƒä¸­è¿è¡Œ")
                print("2. GPUé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
                print("3. PaddlePaddleæ˜¯å¦ä¸ºGPUç‰ˆæœ¬")
                print("=" * 60)
                raise SystemExit("ç¨‹åºç»ˆæ­¢ï¼šå¿…é¡»ä½¿ç”¨GPUç¯å¢ƒ")
    
    def setup_seeds(self):
        """è®¾ç½®éšæœºç§å­"""
        paddle.seed(42)
        np.random.seed(42)
        random.seed(42)
    
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            'trained_models',
            'training_logs',
            'training_results',
            'training_curves'
        ]
        for dir_name in directories:
            os.makedirs(dir_name, exist_ok=True)
    
    def create_model(self, model_type='simple', learning_rate=0.001):
        """åˆ›å»ºæ¨¡å‹"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤2: åˆ›å»ºCNNæ¨¡å‹")
        print("=" * 60)
        
        # å¼ºåˆ¶ä½¿ç”¨ç®€å•æ¨¡å‹é˜²æ­¢è¿‡æ‹Ÿåˆ
        if model_type != 'simple':
            print(f"âš ï¸  è‡ªåŠ¨å°†æ¨¡å‹ç±»å‹ä» '{model_type}' æ”¹ä¸º 'simple' é˜²æ­¢è¿‡æ‹Ÿåˆ")
            model_type = 'simple'
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_model_by_type(model_type=model_type)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if not p.stop_gradient)
        
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
        print(f"æ€»å‚æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ·»åŠ æƒé‡è¡°å‡
        self.optimizer = optim.AdamW(
            learning_rate=learning_rate,
            parameters=self.model.parameters(),
            weight_decay=0.0005,  # æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
            beta1=0.9,
            beta2=0.999,
            epsilon=1e-8,
            grad_clip=nn.ClipGradByGlobalNorm(clip_norm=1.0)  # æ¢¯åº¦è£å‰ª
        )
        
        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = CosineAnnealingDecay(
            learning_rate=learning_rate,
            T_max=30,  # æ€»è®­ç»ƒè½®æ•°
            eta_min=learning_rate * 0.01  # æœ€å°å­¦ä¹ ç‡
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        print(f"ä¼˜åŒ–å™¨: AdamW with weight_decay=0.0005")
        print(f"å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealingDecay")
        print(f"åˆå§‹å­¦ä¹ ç‡: {learning_rate}")
        print(f"âš ï¸  å·²å¯ç”¨é˜²è¿‡æ‹Ÿåˆæªæ–½: L2æ­£åˆ™åŒ– + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦è£å‰ª")
        
        return self.model
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(train_loader, desc='è®­ç»ƒ', leave=False)
        
        for batch_idx, (data, target) in enumerate(progress_bar):
            # å‰å‘ä¼ æ’­
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            self.optimizer.clear_grad()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            predicted = output.argmax(axis=1)
            total += target.shape[0]
            batch_correct = (predicted == target).sum().item()
            correct += batch_correct
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{batch_correct/target.shape[0]:.2%}'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total if total > 0 else 0
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(val_loader, desc='éªŒè¯', leave=False)
        
        with paddle.no_grad():
            for data, target in progress_bar:
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                predicted = output.argmax(axis=1)
                total += target.shape[0]
                batch_correct = (predicted == target).sum().item()
                correct += batch_correct
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{batch_correct/target.shape[0]:.2%}'
                })
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total if total > 0 else 0
        
        return avg_loss, accuracy
    
    def train_model(self, epochs=50, batch_size=32, model_type='simple', 
                   learning_rate=0.001, optimizer_type='adam'):
        """è®­ç»ƒæ¨¡å‹"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤3: è®­ç»ƒæ¨¡å‹ (GPUåŠ é€Ÿ)")
        print("=" * 60)
        print("ğŸš¨ æ­£åœ¨åº”ç”¨è¿‡æ‹Ÿåˆä¿®å¤æ–¹æ¡ˆ...")
        print("   1. å¼ºåˆ¶ä½¿ç”¨ç®€å•æ¨¡å‹")
        print("   2. æ·»åŠ L2æ­£åˆ™åŒ– (weight_decay=0.0005)")
        print("   3. æ·»åŠ ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦")
        print("   4. æ¢¯åº¦è£å‰ª (clip_norm=1.0)")
        print("   5. å¼ºåˆ¶GPUåŠ é€Ÿè®­ç»ƒ")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        train_loader, val_loader, test_loader, data_info = create_data_loaders(
            batch_size=batch_size,
            augment_train=True  # å¯ç”¨æ•°æ®å¢å¼º
        )
        
        if train_loader is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
            return None
        
        X_train, y_train, X_val, y_val, X_test, y_test = data_info
        
        print(f"\nğŸ” æ•°æ®æ£€æŸ¥:")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°é‡: {len(X_train)}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°é‡: {len(X_val)}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°é‡: {len(X_test)}")
        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
        print(f"éªŒè¯æ‰¹æ¬¡æ•°é‡: {len(val_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        self.create_model(model_type=model_type, learning_rate=learning_rate)
        
        # GPUæ€§èƒ½æµ‹è¯•
        print(f"\nâš¡ GPUæ€§èƒ½æµ‹è¯•...")
        start_test = datetime.now()
        test_tensor = paddle.randn([128, 3, 64, 64])
        test_result = test_tensor * 2.0
        end_test = datetime.now()
        test_time = (end_test - start_test).total_seconds()
        print(f"GPUå¼ é‡è¿ç®—æµ‹è¯•: {test_time:.4f}ç§’")
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        checkpoint_path = f'trained_models/{self.model_name}_best_{self.timestamp}.pdparams'
        checkpoint = PaddleModelCheckpoint(
            filepath=checkpoint_path,
            monitor='val_acc',
            save_best_only=True,
            mode='max',
            verbose=1
        )
        checkpoint.set_model(self.model)
        
        # æå‰åœæ­¢å›è°ƒ
        early_stopping = PaddleEarlyStopping(
            monitor='val_acc',
            patience=15,
            min_delta=0.001,
            restore_best_weights=True,
            verbose=1
        )
        early_stopping.set_model(self.model)
        
        print(f"\nğŸš€ å¼€å§‹GPUåŠ é€Ÿè®­ç»ƒ (æ€»è½®æ•°: {epochs})...")
        print(f"ğŸ“Š å½“å‰è®¾å¤‡: {paddle.device.get_device()}")
        print("-" * 60)
        
        start_time = datetime.now()
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.scheduler.get_lr()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['lr'].append(current_lr)
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            logs = {'val_acc': val_acc, 'val_loss': val_loss}
            checkpoint.on_epoch_end(epoch, logs)
            
            # æ£€æŸ¥æ˜¯å¦æå‰åœæ­¢
            if early_stopping.on_epoch_end(epoch, logs):
                print(f"âš ï¸  æå‰åœæ­¢è®­ç»ƒ")
                break
            
            # æ‰“å°è¿›åº¦
            print(f"  è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc:.2%})")
            print(f"  éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc:.2%})")
            print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # æ£€æŸ¥è¿‡æ‹Ÿåˆè­¦å‘Š
            if epoch >= 5:  # è‡³å°‘è®­ç»ƒ5è½®åå†æ£€æŸ¥
                gap = train_acc - val_acc
                if gap > 0.3:
                    print(f"âš ï¸  ä¸¥é‡è¿‡æ‹Ÿåˆè­¦å‘Š: è®­ç»ƒ-éªŒè¯å·®è·={gap:.4f}")
                elif gap > 0.2:
                    print(f"âš ï¸  ä¸­åº¦è¿‡æ‹Ÿåˆè­¦å‘Š: è®­ç»ƒ-éªŒè¯å·®è·={gap:.4f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
        
        # è®­ç»ƒå®Œæˆ
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print("âœ… GPUè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"å¹³å‡æ¯è½®: {training_time/len(self.history['train_loss']):.2f}ç§’")
        
        # åˆ†æè¿‡æ‹Ÿåˆç¨‹åº¦
        self.analyze_overfitting()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = f'trained_models/{self.model_name}_best_{self.timestamp}.pdparams'
        if os.path.exists(best_model_path):
            print(f"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹...")
            self.model.set_state_dict(paddle.load(best_model_path))
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model()
        
        # è¯„ä¼°æ¨¡å‹
        test_acc = self.evaluate_model(test_loader)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        self.save_training_report(test_acc, len(X_test), training_time)
        
        return self.history
    
    def analyze_overfitting(self):
        """åˆ†æè¿‡æ‹Ÿåˆç¨‹åº¦"""
        if not self.history['train_acc']:
            return
        
        final_train_acc = self.history['train_acc'][-1]
        final_val_acc = self.history['val_acc'][-1]
        gap = final_train_acc - final_val_acc
        
        # æ‰¾åˆ°æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_val_acc = max(self.history['val_acc'])
        best_val_epoch = self.history['val_acc'].index(best_val_acc) + 1
        
        print(f"\nğŸ“Š è¿‡æ‹Ÿåˆåˆ†æ:")
        print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f} ({final_train_acc:.2%})")
        print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f} ({final_val_acc:.2%})")
        print(f"  è®­ç»ƒ-éªŒè¯å·®è·: {gap:.4f}")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} ({best_val_acc:.2%}) - ç¬¬{best_val_epoch}è½®")
        
        if gap > 0.3:
            print(f"  ğŸ”´ ä¸¥é‡è¿‡æ‹Ÿåˆï¼å»ºè®®:")
            print(f"    1. è¿›ä¸€æ­¥å¢åŠ æƒé‡è¡°å‡åˆ°0.001")
            print(f"    2. å¢åŠ Dropoutç‡")
            print(f"    3. å‡å°‘è®­ç»ƒè½®æ•°")
        elif gap > 0.2:
            print(f"  ğŸŸ¡ ä¸­ç­‰è¿‡æ‹Ÿåˆ")
        elif gap > 0.1:
            print(f"  ğŸŸ¢ è½»å¾®è¿‡æ‹Ÿåˆ")
        else:
            print(f"  âœ… ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ï¼")
    
    def save_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        model_path = f'trained_models/{self.model_name}_final_{self.timestamp}.pdparams'
        paddle.save(self.model.state_dict(), model_path)
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªç®€å•åç§°çš„å‰¯æœ¬
        simple_path = 'my_traffic_classifier_paddle.pdparams'
        paddle.save(self.model.state_dict(), simple_path)
        
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {model_path}")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜ä¸º: {simple_path}")
        
        return model_path
    
    def evaluate_model(self, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤4: è¯„ä¼°æ¨¡å‹")
        print("=" * 60)
        
        if test_loader is None:
            print("âŒ æµ‹è¯•æ•°æ®åŠ è½½å™¨ä¸ºç©º")
            return 0.0
        
        self.model.eval()
        correct = 0
        total = 0
        test_loss = 0
        
        print("ğŸ“Š è¯„ä¼°æµ‹è¯•é›†...")
        
        with paddle.no_grad():
            for data, target in tqdm(test_loader, desc='æµ‹è¯•'):
                output = self.model(data)
                loss = self.criterion(output, target)
                
                test_loss += loss.item()
                predicted = output.argmax(axis=1)
                total += target.shape[0]
                correct += (predicted == target).sum().item()
        
        avg_test_loss = test_loss / len(test_loader)
        test_acc = correct / total if total > 0 else 0
        
        print(f"æµ‹è¯•é›†æŸå¤±: {avg_test_loss:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc:.2%})")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {total}")
        
        return test_acc
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.history['train_acc'] or len(self.history['train_acc']) < 2:
            print("è®­ç»ƒå†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»˜åˆ¶æ›²çº¿")
            return
        
        plt.figure(figsize=(15, 5))
        
        # 1. å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 3, 1)
        epochs = range(1, len(self.history['train_acc']) + 1)
        
        plt.plot(epochs, self.history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        plt.plot(epochs, self.history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        
        # æ ‡è®°æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_val_acc = max(self.history['val_acc'])
        best_epoch = self.history['val_acc'].index(best_val_acc)
        plt.scatter(best_epoch + 1, best_val_acc, color='red', s=100, zorder=5)
        plt.text(best_epoch + 1, best_val_acc - 0.05, f'æœ€ä½³: {best_val_acc:.3f}', 
                fontsize=10, ha='center', color='red')
        
        plt.title('å‡†ç¡®ç‡æ›²çº¿', fontsize=12, fontweight='bold')
        plt.xlabel('è®­ç»ƒè½®æ•°', fontsize=11)
        plt.ylabel('å‡†ç¡®ç‡', fontsize=11)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.ylim([0, 1.0])
        
        # 2. æŸå¤±æ›²çº¿
        plt.subplot(1, 3, 2)
        plt.plot(epochs, self.history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        plt.plot(epochs, self.history['val_loss'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        plt.title('æŸå¤±æ›²çº¿', fontsize=12, fontweight='bold')
        plt.xlabel('è®­ç»ƒè½®æ•°', fontsize=11)
        plt.ylabel('æŸå¤±', fontsize=11)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # 3. å­¦ä¹ ç‡æ›²çº¿
        plt.subplot(1, 3, 3)
        plt.plot(epochs, self.history['lr'], 'g-', label='å­¦ä¹ ç‡', linewidth=2)
        plt.title('å­¦ä¹ ç‡å˜åŒ–', fontsize=12, fontweight='bold')
        plt.xlabel('è®­ç»ƒè½®æ•°', fontsize=11)
        plt.ylabel('å­¦ä¹ ç‡', fontsize=11)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        curve_path = f'training_curves/training_history_{self.timestamp}.png'
        plt.savefig(curve_path, dpi=150, bbox_inches='tight')
        plt.savefig('training_results_paddle.png', dpi=150, bbox_inches='tight')
        
        print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
        plt.show()
    
    def save_training_report(self, test_acc, test_samples, training_time):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        if not self.history['train_acc']:
            print("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        # æ”¶é›†æ•°æ®
        train_acc = self.history['train_acc'][-1]
        val_acc = self.history['val_acc'][-1]
        best_val_acc = max(self.history['val_acc'])
        epochs = len(self.history['train_acc'])
        gap = train_acc - val_acc
        
        # è¯„ä¼°è¿‡æ‹Ÿåˆç¨‹åº¦
        if gap > 0.3:
            overfitting_status = 'ä¸¥é‡è¿‡æ‹Ÿåˆ'
            status_emoji = 'ğŸ”´'
        elif gap > 0.2:
            overfitting_status = 'ä¸­ç­‰è¿‡æ‹Ÿåˆ'
            status_emoji = 'ğŸŸ¡'
        elif gap > 0.1:
            overfitting_status = 'è½»å¾®è¿‡æ‹Ÿåˆ'
            status_emoji = 'ğŸŸ¢'
        else:
            overfitting_status = 'ä¼˜ç§€æ³›åŒ–'
            status_emoji = 'âœ…'
        
        # åˆ›å»ºæŠ¥å‘Š
        report = {
            'model_name': self.model_name,
            'timestamp': self.timestamp,
            'image_size': self.image_size,
            'training_time_seconds': float(training_time),
            'training_epochs': int(epochs),
            'final_train_accuracy': float(train_acc),
            'final_val_accuracy': float(val_acc),
            'train_val_gap': float(gap),
            'best_val_accuracy': float(best_val_acc),
            'test_accuracy': float(test_acc),
            'test_samples': int(test_samples),
            'overfitting_status': overfitting_status,
            'status_emoji': status_emoji,
            'device': str(paddle.device.get_device()),
            'parameters': {
                'batch_size': 32,
                'learning_rate': 0.001,
                'weight_decay': 0.0005,
                'patience': 15
            }
        }
        
        # ä¿å­˜ä¸ºJSON
        report_path = f'training_results/training_report_{self.timestamp}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("è®­ç»ƒæŠ¥å‘Š")
        print("=" * 60)
        print(f"æ¨¡å‹åç§°: {report['model_name']}")
        print(f"è®­ç»ƒæ—¶é—´: {report['timestamp']}")
        print(f"è®­ç»ƒè®¾å¤‡: {report['device']}")
        print(f"å›¾åƒå°ºå¯¸: {report['image_size']}")
        print(f"è®­ç»ƒè½®æ•°: {report['training_epochs']}")
        print(f"è®­ç»ƒç”¨æ—¶: {report['training_time_seconds']:.2f}ç§’")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {report['final_train_accuracy']:.4f} ({report['final_train_accuracy']:.2%})")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {report['final_val_accuracy']:.4f} ({report['final_val_accuracy']:.2%})")
        print(f"è®­ç»ƒ-éªŒè¯å·®è·: {report['train_val_gap']:.4f} - {status_emoji} {overfitting_status}")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {report['best_val_accuracy']:.4f} ({report['best_val_accuracy']:.2%})")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {report['test_accuracy']:.4f} ({report['test_accuracy']:.2%})")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {report['test_samples']}")
        
        # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
        print(f"\n{status_emoji} å»ºè®®:")
        if overfitting_status == 'ä¸¥é‡è¿‡æ‹Ÿåˆ':
            print("  1. è¿›ä¸€æ­¥å¢åŠ æƒé‡è¡°å‡åˆ°0.001")
            print("  2. å¢åŠ Dropoutç‡")
            print("  3. ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹")
            print("  4. å¢åŠ æ•°æ®å¢å¼ºå¼ºåº¦")
        elif overfitting_status == 'ä¸­ç­‰è¿‡æ‹Ÿåˆ':
            print("  1. ç•¥å¾®å¢åŠ æƒé‡è¡°å‡")
            print("  2. å¢åŠ æ—©åœè€å¿ƒå€¼")
            print("  3. è°ƒæ•´æ•°æ®å¢å¼ºå‚æ•°")
        elif overfitting_status == 'è½»å¾®è¿‡æ‹Ÿåˆ':
            print("  1. æ¨¡å‹çŠ¶æ€è‰¯å¥½")
            print("  2. å¯ä»¥å°è¯•è°ƒæ•´å­¦ä¹ ç‡")
        else:
            print("  1. æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¼˜ç§€")
            print("  2. å¯ä»¥å°è¯•å¾®è°ƒå‚æ•°è·å¾—æ›´å¥½ç»“æœ")
        
        print(f"\nâœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("å¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ« - CNNæ¨¡å‹è®­ç»ƒ (å¼ºåˆ¶GPUåŠ é€Ÿç‰ˆ)")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CNNTrainerPaddle(
        model_name='traffic_sign_cnn_paddle',
        image_size=(64, 64)
    )
    
    # è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ç¨³å®šå‚æ•°
    history = trainer.train_model(
        epochs=50,  # å¢åŠ æœ€å¤§è½®æ•°ï¼Œä½†ä¼šæœ‰æ—©åœ
        batch_size=32,
        model_type='simple',  # å¼ºåˆ¶ä½¿ç”¨ç®€å•æ¨¡å‹
        learning_rate=0.001,
        optimizer_type='adam'
    )
    
    if history is not None:
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print("=" * 60)
        print("å¼ºåˆ¶GPUæ¨¡å¼æ€»ç»“:")
        print("âœ… 1. å¼ºåˆ¶ä½¿ç”¨GPUï¼Œä¸ä¼šé€€å›åˆ°CPU")
        print("âœ… 2. å¦‚æœGPUä¸å¯ç”¨ï¼Œç¨‹åºä¼šæŠ¥é”™é€€å‡º")
        print("âœ… 3. ä½¿ç”¨ä¾å›¾GPUåŠ é€Ÿå¡ (iluvatar_gpu:0)")
        print("=" * 60)
        print("è¿‡æ‹Ÿåˆä¿®å¤æªæ–½:")
        print("âœ… 1. å¼ºåˆ¶ä½¿ç”¨ç®€å•æ¨¡å‹ (å‡å°‘å‚æ•°)")
        print("âœ… 2. æ·»åŠ L2æ­£åˆ™åŒ– (weight_decay=0.0005)")
        print("âœ… 3. æ·»åŠ ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦")
        print("âœ… 4. æ¢¯åº¦è£å‰ª (clip_norm=1.0)")
        print("âœ… 5. æ”¹è¿›æ—©åœç­–ç•¥ (patience=15)")
        print("âœ… 6. éšæœºç§å­è®¾ç½® (ç¡®ä¿å¯é‡å¤æ€§)")
        print("=" * 60)
        print("è¾“å‡ºæ–‡ä»¶:")
        print("1. è®­ç»ƒæ›²çº¿: training_results_paddle.png")
        print("2. è¯¦ç»†æ›²çº¿: training_curves/ ç›®å½•")
        print("3. è®­ç»ƒæŠ¥å‘Š: training_results/ ç›®å½•")
        print("4. æœ€ä½³æ¨¡å‹: trained_models/ ç›®å½•")
        print("5. ç®€ç‰ˆæ¨¡å‹: my_traffic_classifier_paddle.pdparams")
        print("=" * 60)
        print("ä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿åˆ†ææ¨¡å‹è¡¨ç°")
        print("2. ä½¿ç”¨æµ‹è¯•é›†éªŒè¯æ¨¡å‹æ€§èƒ½")
        print("3. æ ¹æ®æŠ¥å‘Šè°ƒæ•´å‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–")
        print("=" * 60)

if __name__ == "__main__":
    print(f"PaddlePaddleç‰ˆæœ¬: {paddle.__version__}")
    print(f"NumPyç‰ˆæœ¬: {np.__version__}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists('processed_data'):
        print("ğŸ“ åˆ›å»ºprocessed_dataç›®å½•...")
        os.makedirs('processed_data', exist_ok=True)
        print("å°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•")
    
    # è¿è¡Œä¸»ç¨‹åº
    main()