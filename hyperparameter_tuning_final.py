"""
hyperparameter_tuning_final.py - è¶…å‚æ•°è°ƒä¼˜ä¸äº¤å‰éªŒè¯ï¼ˆå¤ç”¨A&Bä»£ç ï¼‰
æˆå‘˜Cä»»åŠ¡ï¼šåŸºäºæˆå‘˜Açš„æ•°æ®é¢„å¤„ç†å’Œæˆå‘˜Bçš„CNNæ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
"""

import numpy as np
import tensorflow as tf
from keras.utils import to_categorical
from keras.optimizers import Adam, SGD, RMSprop
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False
import pandas as pd
import time
import os
import json
import warnings

warnings.filterwarnings('ignore')

print("=" * 60)
print("ğŸš¦ å¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ« - è¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿ")
print("âœ… å¤ç”¨ï¼šæˆå‘˜Açš„æ•°æ®é¢„å¤„ç† + æˆå‘˜Bçš„CNNæ¨¡å‹")
print("=" * 60)

# ==================== å¼ºåˆ¶å¤ç”¨ç°æœ‰ä»£ç  ====================
try:
    # 1. å¤ç”¨æˆå‘˜Açš„æ•°æ®é¢„å¤„ç†æ¨¡å—
    from data_preprocessing import GTSRBDataLoader

    print("âœ… æˆåŠŸå¯¼å…¥ GTSRBDataLoader (æˆå‘˜A)")

    # 2. å¤ç”¨æˆå‘˜Bçš„CNNæ¨¡å‹æ¨¡å—
    from cnn_model import create_traffic_cnn_model, create_simple_cnn_model, create_reference_model

    print("âœ… æˆåŠŸå¯¼å…¥ CNNæ¨¡å‹å‡½æ•° (æˆå‘˜B)")

except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š")
    print("  1. data_preprocessing.py - æˆå‘˜Açš„æ•°æ®é¢„å¤„ç†")
    print("  2. cnn_model.py - æˆå‘˜Bçš„CNNæ¨¡å‹")
    print("  3. processed_data/ - é¢„å¤„ç†æ•°æ®ç›®å½•")
    exit(1)

print("=" * 60)


class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨ - å®Œå…¨å¤ç”¨ç°æœ‰ä»£ç """

    def __init__(self, model_type='standard'):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        model_type: 'standard' (æ ‡å‡†CNN), 'simple', 'reference'
        """
        self.model_type = model_type
        self.best_params = None
        self.best_score = 0
        self.results = []

        # åˆ›å»ºç»“æœç›®å½• - ä¿®æ”¹ä¸º hyperparameter_tuning_result
        os.makedirs('hyperparameter_tuning_result', exist_ok=True)

        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")

    def load_data_from_processed(self):
        """
        å¤ç”¨æˆå‘˜Açš„é¢„å¤„ç†æ•°æ®
        ç›´æ¥åŠ è½½ processed_data ç›®å½•ä¸­çš„æ•°æ®
        """
        print("\nğŸ“‚ åŠ è½½é¢„å¤„ç†æ•°æ®...")

        try:
            # ç›´æ¥åŠ è½½ .npy æ–‡ä»¶ï¼ˆæœ€å¿«æœ€ç®€å•ï¼‰
            X_train = np.load('processed_data/X_train.npy')
            X_val = np.load('processed_data/X_val.npy')
            X_test = np.load('processed_data/X_test.npy')
            y_train = np.load('processed_data/y_train.npy')
            y_val = np.load('processed_data/y_val.npy')
            y_test = np.load('processed_data/y_test.npy')

            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
            print(f"  è®­ç»ƒé›†: {X_train.shape} - {len(y_train)} æ ·æœ¬")
            print(f"  éªŒè¯é›†: {X_val.shape} - {len(y_val)} æ ·æœ¬")
            print(f"  æµ‹è¯•é›†: {X_test.shape} - {len(y_test)} æ ·æœ¬")

            # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†ç”¨äºäº¤å‰éªŒè¯
            X_full = np.concatenate([X_train, X_val], axis=0)
            y_full = np.concatenate([y_train, y_val], axis=0)

            return X_full, y_full, X_test, y_test

        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ processed_data/ ç›®å½•åŒ…å«æ‰€éœ€æ–‡ä»¶")
            exit(1)

    def create_model_with_params(self, params):
        """
        å¤ç”¨æˆå‘˜Bçš„CNNæ¨¡å‹ï¼Œä½†å…è®¸å‚æ•°è°ƒæ•´ï¼ˆåŒ…æ‹¬Dropoutç‡ï¼‰
        params: åŒ…å«è¶…å‚æ•°çš„å­—å…¸
        """
        from keras.models import Sequential
        from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation

        input_shape = (64, 64, 3)  # å›ºå®šï¼Œä¸é¢„å¤„ç†ä¸€è‡´
        num_classes = 43

        # æå–Dropoutå‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        conv_dropout = params.get('conv_dropout', 0.25)  # å·ç§¯å±‚Dropout
        fc_dropout = params.get('fc_dropout', 0.5)      # å…¨è¿æ¥å±‚Dropout

        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹æ¶æ„ï¼ˆæ”¯æŒå¯è°ƒDropoutï¼‰
        if self.model_type == 'simple':
            model = self._create_simple_model_with_dropout(input_shape, num_classes, conv_dropout, fc_dropout)
        elif self.model_type == 'reference':
            model = self._create_reference_model_with_dropout(input_shape, num_classes, conv_dropout, fc_dropout)
        else:  # 'standard'
            model = self._create_standard_model_with_dropout(input_shape, num_classes, conv_dropout, fc_dropout)

        # é€‰æ‹©ä¼˜åŒ–å™¨
        optimizer = self._get_optimizer(params)

        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        return model

    def _create_standard_model_with_dropout(self, input_shape, num_classes, conv_dropout, fc_dropout):
        """åˆ›å»ºæ ‡å‡†æ¨¡å‹ï¼ˆæ”¯æŒå¯è°ƒDropoutï¼‰"""
        from keras.models import Sequential
        from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation

        model = Sequential(name="TrafficSignCNN_Tunable")

        # ç¬¬ä¸€å·ç§¯å—
        model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(conv_dropout))

        # ç¬¬äºŒå·ç§¯å—
        model.add(Conv2D(64, (3, 3), padding='same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(conv_dropout))

        # ç¬¬ä¸‰å·ç§¯å—
        model.add(Conv2D(128, (3, 3), padding='same'))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(conv_dropout))

        # å…¨è¿æ¥å±‚
        model.add(Flatten())
        model.add(Dense(512))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Dropout(fc_dropout))

        # è¾“å‡ºå±‚
        model.add(Dense(num_classes, activation='softmax'))

        return model

    def _create_simple_model_with_dropout(self, input_shape, num_classes, conv_dropout, fc_dropout):
        """åˆ›å»ºç®€å•æ¨¡å‹ï¼ˆæ”¯æŒå¯è°ƒDropoutï¼‰"""
        from keras.models import Sequential
        from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation

        model = Sequential(name="SimpleTrafficCNN_Tunable")

        model.add(Conv2D(32, (3, 3), input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D((2, 2)))
        model.add(Dropout(conv_dropout))

        model.add(Conv2D(64, (3, 3)))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D((2, 2)))
        model.add(Dropout(conv_dropout))

        model.add(Flatten())
        model.add(Dense(128))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Dropout(fc_dropout))

        model.add(Dense(num_classes, activation='softmax'))

        return model

    def _create_reference_model_with_dropout(self, input_shape, num_classes, conv_dropout, fc_dropout):
        """åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆæ”¯æŒå¯è°ƒDropoutï¼‰"""
        from keras.models import Sequential
        from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation

        model = Sequential(name="ReferenceWithBatchNorm_Tunable")

        model.add(Conv2D(16, (3, 3), input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(Activation('relu'))

        model.add(Conv2D(32, (3, 3)))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(conv_dropout))

        model.add(Conv2D(64, (3, 3)))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(conv_dropout))

        model.add(Flatten())
        model.add(Dense(512))
        model.add(BatchNormalization())
        model.add(Activation('relu'))
        model.add(Dropout(fc_dropout))

        model.add(Dense(num_classes, activation='softmax'))

        return model

    def _get_optimizer(self, params):
        """è·å–ä¼˜åŒ–å™¨"""
        lr = params.get('learning_rate', 0.001)
        optimizer_type = params.get('optimizer_type', 'adam')

        if optimizer_type.lower() == 'sgd':
            return SGD(learning_rate=lr, momentum=0.9)
        elif optimizer_type.lower() == 'rmsprop':
            return RMSprop(learning_rate=lr)
        else:  # adam
            return Adam(learning_rate=lr)

    def kfold_cross_validation(self, n_splits=5, batch_size=32, epochs=10):
        """
        KæŠ˜äº¤å‰éªŒè¯ - è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
        å®Œå…¨å¤ç”¨æˆå‘˜Bçš„æ¨¡å‹æ¶æ„
        """
        print("\n" + "=" * 60)
        print(f"ğŸ“Š {n_splits}-æŠ˜äº¤å‰éªŒè¯")
        print(f"è¯„ä¼°æ¨¡å‹: {self.model_type}")
        print(f"é…ç½®: epochs={epochs}, batch_size={batch_size}")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        X, y, X_test, y_test = self.load_data_from_processed()
        y_onehot = to_categorical(y, 43)

        # åˆ›å»ºKFold
        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)

        fold_scores = []
        fold_histories = []

        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):
            print(f"\nğŸ”„ Fold {fold + 1}/{n_splits}")
            start_time = time.time()

            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]

            # ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹ï¼ˆå¤ç”¨Bçš„ä»£ç ï¼‰
            model = self.create_model_with_params({
                'learning_rate': 0.001,
                'optimizer_type': 'adam'
            })

            # è®­ç»ƒ
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                verbose=0
            )

            # è¯„ä¼°
            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
            fold_time = time.time() - start_time

            fold_scores.append(val_acc)
            fold_histories.append(history.history)

            print(f"  âœ… å‡†ç¡®ç‡: {val_acc:.4f} | æŸå¤±: {val_loss:.4f} | æ—¶é—´: {fold_time:.1f}s")

        # ç»Ÿè®¡ç»“æœ
        self._analyze_kfold_results(fold_scores, fold_histories, n_splits)

        return np.mean(fold_scores), np.std(fold_scores)

    def _analyze_kfold_results(self, fold_scores, fold_histories, n_splits):
        """åˆ†æKæŠ˜ç»“æœ"""
        mean_score = np.mean(fold_scores)
        std_score = np.std(fold_scores)

        print("\n" + "=" * 60)
        print(f"ğŸ“ˆ {n_splits}-æŠ˜äº¤å‰éªŒè¯ç»“æœ")
        print(f"å¹³å‡å‡†ç¡®ç‡: {mean_score:.4f} ({mean_score:.2%})")
        print(f"æ ‡å‡†å·®: {std_score:.4f}")
        print(f"å„æŠ˜å‡†ç¡®ç‡: {[f'{s:.4f}' for s in fold_scores]}")

        # ç»˜åˆ¶ç»“æœ
        self._plot_kfold_results(fold_scores, fold_histories)

        # ä¿å­˜ç»“æœ - ä¿®æ”¹è·¯å¾„
        kfold_results = {
            'n_splits': n_splits,
            'model_type': self.model_type,
            'mean_accuracy': float(mean_score),
            'std_accuracy': float(std_score),
            'fold_accuracies': [float(s) for s in fold_scores],
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        with open('hyperparameter_tuning_result/kfold_results.json', 'w', encoding='utf-8') as f:
            json.dump(kfold_results, f, indent=4, ensure_ascii=False)

    def systematic_search(self, n_folds=3, epochs=5):
        """
        ç³»ç»ŸåŒ–è¶…å‚æ•°æœç´¢ - æµ‹è¯•å…³é”®å‚æ•°ç»„åˆ
        è®¾è®¡å®éªŒï¼šå­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€ä¼˜åŒ–å™¨ã€Dropoutç‡
        """
        print("\n" + "=" * 60)
        print("ğŸ”¬ ç³»ç»ŸåŒ–è¶…å‚æ•°æœç´¢")
        print(f"äº¤å‰éªŒè¯: {n_folds}æŠ˜ | æ¯è½®epochs: {epochs}")
        print(f"æ¨¡å‹: {self.model_type}")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        X, y, X_test, y_test = self.load_data_from_processed()
        y_onehot = to_categorical(y, 43)

        # è®¾è®¡ç³»ç»ŸåŒ–å®éªŒ
        experiments = []
        exp_id = 1

        # å®éªŒç»„1: å­¦ä¹ ç‡è°ƒä¼˜ï¼ˆå›ºå®šå…¶ä»–å‚æ•°ï¼‰
        print("\n[å®éªŒç»„1] å­¦ä¹ ç‡è°ƒä¼˜")
        learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]
        for lr in learning_rates:
            experiments.append({
                'id': exp_id,
                'group': 'learning_rate',
                'learning_rate': lr,
                'batch_size': 32,
                'optimizer_type': 'adam',
                'conv_dropout': 0.25,
                'fc_dropout': 0.5,
                'note': f'å­¦ä¹ ç‡={lr}'
            })
            exp_id += 1

        # å®éªŒç»„2: æ‰¹å¤§å°è°ƒä¼˜
        print("[å®éªŒç»„2] æ‰¹å¤§å°è°ƒä¼˜")
        batch_sizes = [16, 32, 64, 128]
        for bs in batch_sizes:
            experiments.append({
                'id': exp_id,
                'group': 'batch_size',
                'learning_rate': 0.001,
                'batch_size': bs,
                'optimizer_type': 'adam',
                'conv_dropout': 0.25,
                'fc_dropout': 0.5,
                'note': f'æ‰¹å¤§å°={bs}'
            })
            exp_id += 1

        # å®éªŒç»„3: ä¼˜åŒ–å™¨å¯¹æ¯”
        print("[å®éªŒç»„3] ä¼˜åŒ–å™¨å¯¹æ¯”")
        optimizers = ['adam', 'sgd', 'rmsprop']
        for opt in optimizers:
            experiments.append({
                'id': exp_id,
                'group': 'optimizer',
                'learning_rate': 0.001,
                'batch_size': 32,
                'optimizer_type': opt,
                'conv_dropout': 0.25,
                'fc_dropout': 0.5,
                'note': f'ä¼˜åŒ–å™¨={opt}'
            })
            exp_id += 1

        # å®éªŒç»„4: Dropoutç‡è°ƒä¼˜
        print("[å®éªŒç»„4] Dropoutç‡è°ƒä¼˜")
        conv_dropouts = [0.15, 0.2, 0.25, 0.3, 0.35]
        fc_dropouts = [0.3, 0.4, 0.5, 0.6, 0.7]

        for cd in conv_dropouts:
            experiments.append({
                'id': exp_id,
                'group': 'dropout_conv',
                'learning_rate': 0.001,
                'batch_size': 32,
                'optimizer_type': 'adam',
                'conv_dropout': cd,
                'fc_dropout': 0.5,
                'note': f'å·ç§¯Dropout={cd}'
            })
            exp_id += 1

        for fd in fc_dropouts:
            experiments.append({
                'id': exp_id,
                'group': 'dropout_fc',
                'learning_rate': 0.001,
                'batch_size': 32,
                'optimizer_type': 'adam',
                'conv_dropout': 0.25,
                'fc_dropout': fd,
                'note': f'å…¨è¿æ¥Dropout={fd}'
            })
            exp_id += 1

        print(f"\næ€»å…±è®¾è®¡ {len(experiments)} ä¸ªå®éªŒ")

        # è¿è¡Œæ‰€æœ‰å®éªŒ
        results = []
        for exp in experiments:
            print(f"\n{'='*60}")
            print(f"å®éªŒ {exp['id']}/{len(experiments)}: {exp['note']}")
            print(f"{'='*60}")

            # KæŠ˜äº¤å‰éªŒè¯
            fold_scores = []
            kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)

            for fold, (train_idx, val_idx) in enumerate(kfold.split(X), 1):
                print(f"  æŠ˜ {fold}/{n_folds}...", end=' ')
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]

                # åˆ›å»ºæ¨¡å‹
                model = self.create_model_with_params(exp)

                # è®­ç»ƒ
                model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=epochs,
                    batch_size=exp['batch_size'],
                    verbose=0
                )

                # è¯„ä¼°
                _, val_acc = model.evaluate(X_val, y_val, verbose=0)
                fold_scores.append(val_acc)

                # æ¸…ç†å†…å­˜
                del model
                tf.keras.backend.clear_session()

            # è®¡ç®—ç»Ÿè®¡
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)

            exp['mean_accuracy'] = float(mean_score)
            exp['std_accuracy'] = float(std_score)
            exp['fold_accuracies'] = [float(s) for s in fold_scores]

            print(f"\n  âœ… å¹³å‡å‡†ç¡®ç‡: {mean_score:.4f} (Â±{std_score:.4f})")

            results.append(exp)

            # æ›´æ–°æœ€ä½³å‚æ•°
            if mean_score > self.best_score:
                self.best_score = mean_score
                self.best_params = exp.copy()
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³å‚æ•°!")

        # ä¿å­˜å’Œåˆ†æç»“æœ
        self.results = results
        self._save_search_results()
        self._analyze_search_results()

        return self.best_params, self.best_score

    def random_search(self, n_iter=20, n_folds=3, epochs=5):
        """
        éšæœºæœç´¢è¶…å‚æ•°
        æœç´¢ç©ºé—´ï¼šå­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€ä¼˜åŒ–å™¨ã€Dropoutç‡
        """
        print("\n" + "=" * 60)
        print("ğŸ² éšæœºæœç´¢è¶…å‚æ•°è°ƒä¼˜")
        print(f"è¿­ä»£: {n_iter}æ¬¡ | äº¤å‰éªŒè¯: {n_folds}æŠ˜")
        print(f"æ¨¡å‹: {self.model_type} | æ¯è½®epochs: {epochs}")
        print("=" * 60)

        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆåŒ…å«Dropoutç‡ï¼‰
        param_space = {
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],
            'batch_size': [16, 32, 64, 128],
            'optimizer_type': ['adam', 'sgd', 'rmsprop'],
            'conv_dropout': [0.15, 0.2, 0.25, 0.3, 0.35],  # å·ç§¯å±‚Dropoutç‡
            'fc_dropout': [0.3, 0.4, 0.5, 0.6, 0.7]        # å…¨è¿æ¥å±‚Dropoutç‡
        }

        # åŠ è½½æ•°æ®
        X, y, X_test, y_test = self.load_data_from_processed()
        y_onehot = to_categorical(y, 43)

        results = []

        for i in range(n_iter):
            print(f"\nğŸ” è¿­ä»£ {i + 1}/{n_iter}")

            # éšæœºé€‰æ‹©å‚æ•°
            params = {
                'learning_rate': np.random.choice(param_space['learning_rate']),
                'batch_size': np.random.choice(param_space['batch_size']),
                'optimizer_type': np.random.choice(param_space['optimizer_type']),
                'conv_dropout': np.random.choice(param_space['conv_dropout']),
                'fc_dropout': np.random.choice(param_space['fc_dropout']),
                'iteration': i + 1
            }

            print(f"  å‚æ•°: LR={params['learning_rate']}, "
                  f"BS={params['batch_size']}, Opt={params['optimizer_type']}, "
                  f"ConvDrop={params['conv_dropout']}, FCDrop={params['fc_dropout']}")

            # äº¤å‰éªŒè¯è¯„ä¼°
            fold_scores = []
            kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)

            for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]

                # åˆ›å»ºæ¨¡å‹
                model = self.create_model_with_params(params)

                # è®­ç»ƒ
                model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=epochs,
                    batch_size=params['batch_size'],
                    verbose=0
                )

                # è¯„ä¼°
                _, val_acc = model.evaluate(X_val, y_val, verbose=0)
                fold_scores.append(val_acc)

            # è®¡ç®—å¹³å‡åˆ†æ•°
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)

            params['mean_accuracy'] = float(mean_score)
            params['std_accuracy'] = float(std_score)
            params['fold_accuracies'] = [float(s) for s in fold_scores]

            print(f"  ğŸ“Š å¹³å‡å‡†ç¡®ç‡: {mean_score:.4f} (Â±{std_score:.4f})")

            results.append(params)

            # æ›´æ–°æœ€ä½³å‚æ•°
            if mean_score > self.best_score:
                self.best_score = mean_score
                self.best_params = params.copy()
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³å‚æ•°!")

        # ä¿å­˜ç»“æœ
        self.results = results
        self._save_search_results()

        # åˆ†æç»“æœ
        self._analyze_search_results()

        return self.best_params, self.best_score

    def optimized_search(self, n_coarse=10, n_fine=3, n_folds=3, coarse_epochs=3, fine_epochs=10):
        """
        åˆ†å±‚æœç´¢ç­–ç•¥ï¼šå…ˆå¿«é€Ÿç­›é€‰ï¼Œå†ç²¾ç»†è°ƒä¼˜
        è§£å†³åŸç³»ç»Ÿæ€§æœç´¢è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜
        """
        print("\n" + "=" * 60)
        print("ğŸ—ï¸  åˆ†å±‚æœç´¢ç­–ç•¥ (ä¼˜åŒ–ç‰ˆ)")
        print(f"é˜¶æ®µ1: å¿«é€Ÿç­›é€‰ {n_coarse} ä¸ªé…ç½®, {coarse_epochs} epochs")
        print(f"é˜¶æ®µ2: ç²¾ç»†è°ƒä¼˜å‰ {n_fine} ä¸ªé…ç½®, {n_folds}æŠ˜, {fine_epochs} epochs")
        print(f"æ¨¡å‹: {self.model_type}")
        print("=" * 60)

        # é˜¶æ®µ1ï¼šå¿«é€Ÿç­›é€‰
        print("\nğŸ“‹ é˜¶æ®µ1: å¿«é€Ÿç­›é€‰")
        coarse_results = self._coarse_search(n_iter=n_coarse, epochs=coarse_epochs)

        # æŒ‰å‡†ç¡®ç‡æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„å‡ ä¸ªé…ç½®
        coarse_results_sorted = sorted(coarse_results,
                                       key=lambda x: x['mean_accuracy'],
                                       reverse=True)

        print(f"\nğŸ† å¿«é€Ÿç­›é€‰ç»“æœ (å‰{n_fine}ä¸ª):")
        for i, result in enumerate(coarse_results_sorted[:n_fine]):
            print(f"  {i + 1}. å‡†ç¡®ç‡: {result['mean_accuracy']:.4f} | å‚æ•°: {result}")

        # é˜¶æ®µ2ï¼šç²¾ç»†è°ƒä¼˜
        print(f"\nğŸ”¬ é˜¶æ®µ2: ç²¾ç»†è°ƒä¼˜ (å‰{n_fine}ä¸ªé…ç½®)")
        fine_results = self._fine_search(coarse_results_sorted[:n_fine],
                                         n_folds=n_folds,
                                         epochs=fine_epochs)

        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_fine_result = max(fine_results, key=lambda x: x['mean_accuracy'])

        # æ›´æ–°æœ€ä½³å‚æ•°
        self.best_score = best_fine_result['mean_accuracy']
        self.best_params = {k: v for k, v in best_fine_result.items()
                            if k not in ['mean_accuracy', 'std_accuracy', 'fold_accuracies']}

        print(f"\nğŸ‰ åˆ†å±‚æœç´¢å®Œæˆ!")
        print(f"æœ€ä½³å‡†ç¡®ç‡: {self.best_score:.4f} ({self.best_score:.2%})")

        # ä¿å­˜ç»“æœ
        self.results = fine_results
        self._save_search_results()
        self._analyze_search_results()

        return self.best_params, self.best_score

    def _coarse_search(self, n_iter=10, epochs=3):
        """
        å¿«é€Ÿç­›é€‰é˜¶æ®µï¼šå•æŠ˜éªŒè¯ï¼Œå°‘é‡epochs
        """
        # åŠ è½½æ•°æ®
        X, y, X_test, y_test = self.load_data_from_processed()
        y_onehot = to_categorical(y, 43)

        # å®šä¹‰æœç´¢ç©ºé—´ï¼ˆä¸random_searchç›¸åŒï¼‰
        param_space = {
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],
            'batch_size': [16, 32, 64, 128],
            'optimizer_type': ['adam', 'sgd', 'rmsprop'],
            'conv_dropout': [0.15, 0.2, 0.25, 0.3, 0.35],
            'fc_dropout': [0.3, 0.4, 0.5, 0.6, 0.7]
        }

        coarse_results = []

        for i in range(n_iter):
            print(f"  å¿«é€Ÿæµ‹è¯• {i + 1}/{n_iter}...", end=' ')

            # éšæœºé€‰æ‹©å‚æ•°
            params = {
                'learning_rate': np.random.choice(param_space['learning_rate']),
                'batch_size': np.random.choice(param_space['batch_size']),
                'optimizer_type': np.random.choice(param_space['optimizer_type']),
                'conv_dropout': np.random.choice(param_space['conv_dropout']),
                'fc_dropout': np.random.choice(param_space['fc_dropout']),
                'iteration': i + 1
            }

            # å•æŠ˜å¿«é€Ÿè¯„ä¼°ï¼ˆè®­ç»ƒé›†80%ï¼ŒéªŒè¯é›†20%ï¼‰
            from sklearn.model_selection import train_test_split
            X_train, X_val, y_train, y_val = train_test_split(
                X, y_onehot, test_size=0.2, random_state=42
            )

            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            model = self.create_model_with_params(params)
            model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=params['batch_size'],
                verbose=0
            )

            # è¯„ä¼°
            _, val_acc = model.evaluate(X_val, y_val, verbose=0)

            params['mean_accuracy'] = float(val_acc)
            params['std_accuracy'] = 0.0  # å•æŠ˜æ²¡æœ‰æ ‡å‡†å·®
            params['fold_accuracies'] = [float(val_acc)]

            coarse_results.append(params)

            print(f"å‡†ç¡®ç‡: {val_acc:.4f}")

            # æ¸…ç†å†…å­˜
            del model
            tf.keras.backend.clear_session()

        return coarse_results

    def _fine_search(self, coarse_results, n_folds=3, epochs=10):
        """
        ç²¾ç»†è°ƒä¼˜é˜¶æ®µï¼šå¤šæŠ˜äº¤å‰éªŒè¯ï¼Œæ›´å¤šepochs
        """
        # åŠ è½½æ•°æ®
        X, y, X_test, y_test = self.load_data_from_processed()
        y_onehot = to_categorical(y, 43)

        fine_results = []

        for i, coarse_params in enumerate(coarse_results):
            print(f"  ç²¾ç»†è°ƒä¼˜é…ç½® {i + 1}/{len(coarse_results)}...")

            # å‡†å¤‡å‚æ•°ï¼ˆç§»é™¤è¿­ä»£ä¿¡æ¯ï¼‰
            params = {k: v for k, v in coarse_params.items()
                      if k not in ['mean_accuracy', 'std_accuracy', 'fold_accuracies', 'iteration']}

            # KæŠ˜äº¤å‰éªŒè¯
            fold_scores = []
            kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)

            for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]

                # åˆ›å»ºæ¨¡å‹
                model = self.create_model_with_params(params)

                # è®­ç»ƒ
                model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=epochs,
                    batch_size=params['batch_size'],
                    verbose=0
                )

                # è¯„ä¼°
                _, val_acc = model.evaluate(X_val, y_val, verbose=0)
                fold_scores.append(val_acc)

                # æ¸…ç†å†…å­˜
                del model
                tf.keras.backend.clear_session()

            # è®¡ç®—ç»Ÿè®¡
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)

            params['mean_accuracy'] = float(mean_score)
            params['std_accuracy'] = float(std_score)
            params['fold_accuracies'] = [float(s) for s in fold_scores]
            params['original_rank'] = i + 1

            print(f"    â†’ å‡†ç¡®ç‡: {mean_score:.4f} (Â±{std_score:.4f})")

            fine_results.append(params)

        return fine_results

    def _save_search_results(self):
        """ä¿å­˜æœç´¢ç»“æœ"""
        # æ’åºç»“æœ
        sorted_results = sorted(self.results, key=lambda x: x['mean_accuracy'], reverse=True)

        # ä¿å­˜ä¸ºJSON - ä¿®æ”¹è·¯å¾„
        for result in sorted_results:
            for key, value in result.items():
                if isinstance(value, (np.integer, np.int64, np.int32)):
                    result[key] = int(value)
                elif isinstance(value, (np.floating, np.float64, np.float32)):
                    result[key] = float(value)
                elif isinstance(value, np.ndarray):
                    result[key] = value.tolist()

            # åŒæ ·å¤„ç†æœ€ä½³å‚æ•°
        if self.best_params:
            for key, value in self.best_params.items():
                if isinstance(value, (np.integer, np.int64, np.int32)):
                    self.best_params[key] = int(value)
                elif isinstance(value, (np.floating, np.float64, np.float32)):
                    self.best_params[key] = float(value)
                elif isinstance(value, np.ndarray):
                    self.best_params[key] = value.tolist()

        self.best_score = float(self.best_score) if self.best_score else 0.0

        # ä¿å­˜ä¸ºJSON - ä¿®æ”¹è·¯å¾„
        search_results = {
            'model_type': str(self.model_type),
            'best_params': self.best_params,
            'best_score': float(self.best_score) if self.best_score else 0.0,
            'all_results': sorted_results,
            'summary': {
                'total_iterations': int(len(self.results)),
                'mean_best_score': float(
                    np.mean([r['mean_accuracy'] for r in sorted_results[:5]]) if sorted_results else 0.0),
                'timestamp': str(time.strftime("%Y-%m-%d %H:%M:%S"))
            }
        }

        with open('hyperparameter_tuning_result/random_search_results.json', 'w', encoding='utf-8') as f:
            json.dump(search_results, f, indent=4, ensure_ascii=False)

        # ä¿å­˜ä¸ºCSV - ä¿®æ”¹è·¯å¾„
        df = pd.DataFrame(sorted_results)
        df.to_csv('hyperparameter_tuning_result/search_results.csv', index=False)

        print(f"\nâœ… æœç´¢ç»“æœå·²ä¿å­˜:")
        print(f"  hyperparameter_tuning_result/random_search_results.json")
        print(f"  hyperparameter_tuning_result/search_results.csv")

    def _analyze_search_results(self):
        """åˆ†ææœç´¢ç»“æœ"""
        if not self.results:
            return

        df = pd.DataFrame(self.results)

        print("\n" + "=" * 60)
        print("ğŸ“Š è¶…å‚æ•°é‡è¦æ€§åˆ†æ")
        print("=" * 60)

        # 1. æœ€ä½³å‚æ•°
        print(f"\nğŸ† æœ€ä½³è¶…å‚æ•°ç»„åˆ:")
        print(f"  å­¦ä¹ ç‡: {self.best_params['learning_rate']}")
        print(f"  æ‰¹å¤§å°: {self.best_params['batch_size']}")
        print(f"  ä¼˜åŒ–å™¨: {self.best_params['optimizer_type']}")
        print(f"  å·ç§¯å±‚Dropout: {self.best_params.get('conv_dropout', 0.25)}")
        print(f"  å…¨è¿æ¥å±‚Dropout: {self.best_params.get('fc_dropout', 0.5)}")
        print(f"  å‡†ç¡®ç‡: {self.best_score:.4f} ({self.best_score:.2%})")
        print(f"  æ ‡å‡†å·®: {self.best_params.get('std_accuracy', 0):.4f}")

        # 2. å‚æ•°å½±å“åˆ†æ
        print(f"\nğŸ“ˆ è¶…å‚æ•°å½±å“åˆ†æ:")

        # å­¦ä¹ ç‡å½±å“
        if 'learning_rate' in df.columns:
            lr_groups = df.groupby('learning_rate')['mean_accuracy'].agg(['mean', 'std', 'count'])
            print(f"  å­¦ä¹ ç‡å½±å“:")
            for lr, stats in lr_groups.iterrows():
                print(f"    {lr}: {stats['mean']:.4f} (n={stats['count']})")

        # æ‰¹å¤§å°å½±å“
        if 'batch_size' in df.columns:
            bs_groups = df.groupby('batch_size')['mean_accuracy'].agg(['mean', 'std', 'count'])
            print(f"  æ‰¹å¤§å°å½±å“:")
            for bs, stats in bs_groups.iterrows():
                print(f"    {bs}: {stats['mean']:.4f} (n={stats['count']})")

        # ä¼˜åŒ–å™¨å½±å“
        if 'optimizer_type' in df.columns:
            opt_groups = df.groupby('optimizer_type')['mean_accuracy'].agg(['mean', 'std', 'count'])
            print(f"  ä¼˜åŒ–å™¨å½±å“:")
            for opt, stats in opt_groups.iterrows():
                print(f"    {opt}: {stats['mean']:.4f} (n={stats['count']})")

        # ç»˜åˆ¶å¯è§†åŒ–
        self._plot_search_analysis(df)

    def _plot_kfold_results(self, fold_scores, fold_histories):
        """ç»˜åˆ¶KæŠ˜ç»“æœ"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # 1. å„æŠ˜å‡†ç¡®ç‡
        axes[0].bar(range(1, len(fold_scores) + 1), fold_scores, color='skyblue', alpha=0.8)
        axes[0].axhline(y=np.mean(fold_scores), color='red', linestyle='--',
                        label=f'å¹³å‡: {np.mean(fold_scores):.4f}')
        axes[0].set_xlabel('Fold')
        axes[0].set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        axes[0].set_title(f'{len(fold_scores)}-æŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # 2. è®­ç»ƒæ›²çº¿
        axes[1].set_title('å„æŠ˜éªŒè¯å‡†ç¡®ç‡æ›²çº¿')
        colors = plt.cm.Set2(np.linspace(0, 1, len(fold_histories)))

        for i, history in enumerate(fold_histories):
            axes[1].plot(history['val_accuracy'], label=f'Fold {i + 1}',
                         color=colors[i], linewidth=1.5, alpha=0.7)

        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        axes[1].legend(loc='lower right', fontsize=9)
        axes[1].grid(True, alpha=0.3)

        plt.suptitle(f'KæŠ˜äº¤å‰éªŒè¯åˆ†æ - {self.model_type}æ¨¡å‹', fontsize=14, fontweight='bold')
        plt.tight_layout()
        # ä¿®æ”¹ä¿å­˜è·¯å¾„
        plt.savefig('hyperparameter_tuning_result/kfold_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()

    def _plot_search_analysis(self, df):
        """ç»˜åˆ¶æœç´¢åˆ†æå›¾"""
        if len(df) < 3:
            return

        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()

        # 1. å­¦ä¹ ç‡ vs å‡†ç¡®ç‡
        if 'learning_rate' in df.columns:
            axes[0].scatter(df['learning_rate'], df['mean_accuracy'], alpha=0.6)
            axes[0].set_xlabel('å­¦ä¹ ç‡')
            axes[0].set_ylabel('å¹³å‡å‡†ç¡®ç‡')
            axes[0].set_title('å­¦ä¹ ç‡å½±å“')
            axes[0].set_xscale('log')
            axes[0].grid(True, alpha=0.3)

        # 2. æ‰¹å¤§å° vs å‡†ç¡®ç‡
        if 'batch_size' in df.columns:
            axes[1].scatter(df['batch_size'], df['mean_accuracy'], alpha=0.6, color='green')
            axes[1].set_xlabel('æ‰¹å¤§å°')
            axes[1].set_ylabel('å¹³å‡å‡†ç¡®ç‡')
            axes[1].set_title('æ‰¹å¤§å°å½±å“')
            axes[1].grid(True, alpha=0.3)

        # 3. ä¼˜åŒ–å™¨å¯¹æ¯”
        if 'optimizer_type' in df.columns:
            optimizer_means = df.groupby('optimizer_type')['mean_accuracy'].mean()
            axes[2].bar(range(len(optimizer_means)), optimizer_means.values,
                        tick_label=optimizer_means.index)
            axes[2].set_xlabel('ä¼˜åŒ–å™¨ç±»å‹')
            axes[2].set_ylabel('å¹³å‡å‡†ç¡®ç‡')
            axes[2].set_title('ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”')
            axes[2].grid(True, alpha=0.3)

        # 4. Dropoutå½±å“åˆ†æ
        if 'conv_dropout' in df.columns and 'fc_dropout' in df.columns:
            scatter = axes[3].scatter(df['conv_dropout'], df['fc_dropout'],
                                     c=df['mean_accuracy'], cmap='viridis',
                                     s=100, alpha=0.6)
            axes[3].set_xlabel('å·ç§¯å±‚Dropoutç‡')
            axes[3].set_ylabel('å…¨è¿æ¥å±‚Dropoutç‡')
            axes[3].set_title('Dropoutç‡ç»„åˆå½±å“ï¼ˆé¢œè‰²=å‡†ç¡®ç‡ï¼‰')
            axes[3].grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=axes[3], label='å‡†ç¡®ç‡')
        else:
            # å¦‚æœæ²¡æœ‰Dropoutæ•°æ®ï¼Œæ˜¾ç¤ºè¿­ä»£è¿›åº¦
            axes[3].plot(df.index, df['mean_accuracy'].sort_values(ascending=False).values,
                         marker='o', linewidth=1.5)
            axes[3].set_xlabel('é…ç½®æ’å')
            axes[3].set_ylabel('å‡†ç¡®ç‡')
            axes[3].set_title('è¶…å‚æ•°é…ç½®æ’åº')
            axes[3].grid(True, alpha=0.3)

        plt.suptitle('è¶…å‚æ•°æœç´¢ç»“æœåˆ†æ', fontsize=14, fontweight='bold')
        plt.tight_layout()
        # ä¿®æ”¹ä¿å­˜è·¯å¾„
        plt.savefig('hyperparameter_tuning_result/search_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()

    def train_final_model_with_best_params(self, epochs=30):
        """
        ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        ä¸æˆå‘˜Båä½œï¼šå°†æœ€ä½³å‚æ•°åº”ç”¨åˆ°æœ€ç»ˆè®­ç»ƒ
        """
        if not self.best_params:
            print("âŒ è¯·å…ˆè¿è¡Œéšæœºæœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°")
            return None

        print("\n" + "=" * 60)
        print("ğŸš€ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print("=" * 60)

        # åŠ è½½å®Œæ•´æ•°æ®
        X_train = np.load('processed_data/X_train.npy')
        X_val = np.load('processed_data/X_val.npy')
        X_test = np.load('processed_data/X_test.npy')
        y_train = np.load('processed_data/y_train.npy')
        y_val = np.load('processed_data/y_val.npy')
        y_test = np.load('processed_data/y_test.npy')

        # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_full = np.concatenate([X_train, X_val], axis=0)
        y_full = np.concatenate([y_train, y_val], axis=0)
        y_full_onehot = to_categorical(y_full, 43)
        y_test_onehot = to_categorical(y_test, 43)

        print(f"è®­ç»ƒæ•°æ®: {X_full.shape}")
        print(f"æµ‹è¯•æ•°æ®: {X_test.shape}")

        print(f"\nğŸ¯ æœ€ä½³å‚æ•°:")
        print(f"  å­¦ä¹ ç‡: {self.best_params['learning_rate']}")
        print(f"  æ‰¹å¤§å°: {self.best_params['batch_size']}")
        print(f"  ä¼˜åŒ–å™¨: {self.best_params['optimizer_type']}")
        print(f"  å·ç§¯å±‚Dropout: {self.best_params.get('conv_dropout', 0.25)}")
        print(f"  å…¨è¿æ¥å±‚Dropout: {self.best_params.get('fc_dropout', 0.5)}")
        print(f"  æ¨¡å‹ç±»å‹: {self.model_type}")

        # åˆ›å»ºæœ€ç»ˆæ¨¡å‹
        model = self.create_model_with_params(self.best_params)

        # è®­ç»ƒ
        print(f"\nâ³ å¼€å§‹è®­ç»ƒ ({epochs} epochs)...")
        start_time = time.time()

        history = model.fit(
            X_full, y_full_onehot,
            validation_data=(X_test, y_test_onehot),
            epochs=epochs,
            batch_size=self.best_params['batch_size'],
            verbose=1
        )

        training_time = time.time() - start_time

        # è¯„ä¼°
        test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)

        print("\n" + "=" * 60)
        print(f"ğŸ¯ æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc:.2%})")
        print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.1f}s")

        # ä¿å­˜æ¨¡å‹
        model_name = f'traffic_sign_model_tuned_{self.model_type}.keras'
        model.save(model_name)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_name}")

        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š - ä¿®æ”¹è·¯å¾„
        self._save_final_report(history, test_acc, training_time)

        return model, test_acc

    def _save_final_report(self, history, test_acc, training_time):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        report = {
            'model_type': self.model_type,
            'best_params': self.best_params,
            'performance': {
                'test_accuracy': float(test_acc),
                'final_val_accuracy': float(history.history['val_accuracy'][-1]),
                'final_train_accuracy': float(history.history['accuracy'][-1]),
                'training_time_seconds': float(training_time)
            },
            'training_history': {
                'train_accuracy': [float(x) for x in history.history['accuracy']],
                'val_accuracy': [float(x) for x in history.history['val_accuracy']],
                'train_loss': [float(x) for x in history.history['loss']],
                'val_loss': [float(x) for x in history.history['val_loss']]
            },
            'recommendations_for_memberB': [
                f"ä½¿ç”¨å­¦ä¹ ç‡: {self.best_params['learning_rate']}",
                f"ä½¿ç”¨æ‰¹å¤§å°: {self.best_params['batch_size']}",
                f"ä½¿ç”¨ä¼˜åŒ–å™¨: {self.best_params['optimizer_type']}",
                f"ä½¿ç”¨{self.model_type}æ¨¡å‹æ¶æ„"
            ],
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # ä¿®æ”¹ä¿å­˜è·¯å¾„
        with open('hyperparameter_tuning_result/final_tuning_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)

        print(f"ğŸ“‹ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: hyperparameter_tuning_result/final_tuning_report.json")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves(history, test_acc)

    def _plot_training_curves(self, history, test_acc):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # å‡†ç¡®ç‡æ›²çº¿
        axes[0].plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        axes[0].plot(history.history['val_accuracy'], label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2)
        axes[0].axhline(y=test_acc, color='green', linestyle='--',
                        label=f'æœ€ç»ˆæµ‹è¯•: {test_acc:.3f}')
        axes[0].set_title('æ¨¡å‹å‡†ç¡®ç‡')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Accuracy')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # æŸå¤±æ›²çº¿
        axes[1].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
        axes[1].plot(history.history['val_loss'], label='æµ‹è¯•æŸå¤±', linewidth=2)
        axes[1].set_title('æ¨¡å‹æŸå¤±')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.suptitle(f'è¶…å‚æ•°è°ƒä¼˜åçš„æ¨¡å‹è®­ç»ƒ - {self.model_type}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        # ä¿®æ”¹ä¿å­˜è·¯å¾„
        plt.savefig('hyperparameter_tuning_result/final_training_curves.png', dpi=150, bbox_inches='tight')
        plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ğŸ¯ å¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ« - è¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿ")
    print("æˆå‘˜Cä»»åŠ¡ï¼šåŸºäºæˆå‘˜A&Bçš„å·¥ä½œè¿›è¡Œæ¨¡å‹ä¼˜åŒ–")
    print("=" * 60)

    # é€‰æ‹©æ¨¡å‹ç±»å‹
    print("\nğŸ“‹ é€‰æ‹©è¦ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„:")
    print("1. standard - æ ‡å‡†CNNæ¨¡å‹ï¼ˆæˆå‘˜Bçš„ä¸»è¦æ¨¡å‹ï¼‰")
    print("2. simple - ç®€å•CNNæ¨¡å‹")
    print("3. reference - å‚è€ƒé¡¹ç›®æ¨¡å‹")

    choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ç±»å‹ (1/2/3, é»˜è®¤1): ").strip()

    model_types = {'1': 'standard', '2': 'simple', '3': 'reference'}
    model_type = model_types.get(choice, 'standard')

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = HyperparameterOptimizer(model_type=model_type)

    while True:
        print("\n" + "=" * 60)
        print("ğŸ“± è¶…å‚æ•°è°ƒä¼˜èœå•")
        print("=" * 60)
        print("1. ğŸ“Š KæŠ˜äº¤å‰éªŒè¯ (è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§)")
        print("2. ğŸ” éšæœºæœç´¢ (å¯»æ‰¾æœ€ä½³è¶…å‚æ•°)")
        print("3. ğŸ”¬ ç³»ç»ŸåŒ–æœç´¢ (ç³»ç»Ÿæµ‹è¯•å…³é”®å‚æ•°)")
        print("4. ğŸ—ï¸  åˆ†å±‚æœç´¢ (æ¨èï¼Œæ›´é«˜æ•ˆ)")
        print("5. ğŸš€ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print("6. ğŸ“ˆ æŸ¥çœ‹å½“å‰æœ€ä½³å‚æ•°")
        print("7. ğŸ“¤ ç”Ÿæˆç»™æˆå‘˜Bçš„å‚æ•°å»ºè®®")
        print("8. ğŸšª é€€å‡º")
        print("=" * 60)

        choice = input("è¯·é€‰æ‹© (1-8): ").strip()

        if choice == '1':
            # KæŠ˜äº¤å‰éªŒè¯
            n_folds = input("æŠ˜æ•° (é»˜è®¤5): ").strip()
            n_folds = int(n_folds) if n_folds else 5

            epochs = input("æ¯æŠ˜è®­ç»ƒè½®æ•° (é»˜è®¤10): ").strip()
            epochs = int(epochs) if epochs else 10

            batch_size = input("æ‰¹å¤§å° (é»˜è®¤32): ").strip()
            batch_size = int(batch_size) if batch_size else 32

            optimizer.kfold_cross_validation(
                n_splits=n_folds,
                epochs=epochs,
                batch_size=batch_size
            )

        elif choice == '2':
            # éšæœºæœç´¢
            n_iter = input("è¿­ä»£æ¬¡æ•° (é»˜è®¤20): ").strip()
            n_iter = int(n_iter) if n_iter else 20

            n_folds = input("äº¤å‰éªŒè¯æŠ˜æ•° (é»˜è®¤3): ").strip()
            n_folds = int(n_folds) if n_folds else 3

            epochs = input("æ¯è½®è®­ç»ƒè½®æ•° (é»˜è®¤5): ").strip()
            epochs = int(epochs) if epochs else 5

            best_params, best_score = optimizer.random_search(
                n_iter=n_iter,
                n_folds=n_folds,
                epochs=epochs
            )

            print(f"\nğŸ† æœ€ä½³å‚æ•°æ‰¾åˆ°! å‡†ç¡®ç‡: {best_score:.4f}")

        elif choice == '3':
            # ç³»ç»ŸåŒ–æœç´¢
            n_folds = input("äº¤å‰éªŒè¯æŠ˜æ•° (é»˜è®¤3): ").strip()
            n_folds = int(n_folds) if n_folds else 3

            epochs = input("æ¯è½®è®­ç»ƒè½®æ•° (é»˜è®¤5): ").strip()
            epochs = int(epochs) if epochs else 5

            best_params, best_score = optimizer.systematic_search(
                n_folds=n_folds,
                epochs=epochs
            )

            print(f"\nğŸ† ç³»ç»ŸåŒ–æœç´¢å®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_score:.4f}")

        elif choice == '4':
            # åˆ†å±‚æœç´¢
            print("\nğŸ—ï¸  åˆ†å±‚æœç´¢ç­–ç•¥ (æ¨è)")
            print("å…ˆå¿«é€Ÿç­›é€‰ï¼Œå†ç²¾ç»†è°ƒä¼˜ï¼ŒèŠ‚çœæ—¶é—´")

            n_coarse = input("å¿«é€Ÿç­›é€‰é…ç½®æ•° (é»˜è®¤10): ").strip()
            n_coarse = int(n_coarse) if n_coarse else 10

            n_fine = input("ç²¾ç»†è°ƒä¼˜é…ç½®æ•° (é»˜è®¤3): ").strip()
            n_fine = int(n_fine) if n_fine else 3

            n_folds = input("ç²¾ç»†è°ƒä¼˜æŠ˜æ•° (é»˜è®¤3): ").strip()
            n_folds = int(n_folds) if n_folds else 3

            coarse_epochs = input("å¿«é€Ÿç­›é€‰epochs (é»˜è®¤3): ").strip()
            coarse_epochs = int(coarse_epochs) if coarse_epochs else 3

            fine_epochs = input("ç²¾ç»†è°ƒä¼˜epochs (é»˜è®¤10): ").strip()
            fine_epochs = int(fine_epochs) if fine_epochs else 10

            best_params, best_score = optimizer.optimized_search(
                n_coarse=n_coarse,
                n_fine=n_fine,
                n_folds=n_folds,
                coarse_epochs=coarse_epochs,
                fine_epochs=fine_epochs
            )

            print(f"\nğŸ† åˆ†å±‚æœç´¢å®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_score:.4f}")

        elif choice == '5':
            # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            if not optimizer.best_params:
                print("è¯·å…ˆè¿è¡Œéšæœºæœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°")
                continue

            epochs = input("è®­ç»ƒè½®æ•° (é»˜è®¤30): ").strip()
            epochs = int(epochs) if epochs else 30

            model, test_acc = optimizer.train_final_model_with_best_params(epochs=epochs)

            if model:
                print(f"\nâœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ! æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2%}")

        elif choice == '6':
            # æŸ¥çœ‹æœ€ä½³å‚æ•°
            if optimizer.best_params:
                print("\nğŸ“‹ å½“å‰æœ€ä½³å‚æ•°:")
                for key, value in optimizer.best_params.items():
                    if key not in ['mean_accuracy', 'std_accuracy', 'fold_accuracies', 'iteration']:
                        print(f"  {key}: {value}")
                print(f"  éªŒè¯å‡†ç¡®ç‡: {optimizer.best_score:.4f}")
            else:
                print("è¿˜æ²¡æœ‰æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œè¯·å…ˆè¿è¡Œéšæœºæœç´¢")

        elif choice == '7':
            # ç”Ÿæˆç»™æˆå‘˜Bçš„å»ºè®®
            if optimizer.best_params:
                print("\nğŸ“¤ ç»™æˆå‘˜Bçš„å‚æ•°å»ºè®®:")
                print("=" * 40)
                print("å»ºè®®åœ¨ train_cnn.py ä¸­ä½¿ç”¨ä»¥ä¸‹å‚æ•°:")
                print("=" * 40)
                print(f"learning_rate = {optimizer.best_params['learning_rate']}")
                print(f"batch_size = {optimizer.best_params['batch_size']}")
                print(f"optimizer = '{optimizer.best_params['optimizer_type']}'")
                print(f"conv_dropout = {optimizer.best_params.get('conv_dropout', 0.25)}")
                print(f"fc_dropout = {optimizer.best_params.get('fc_dropout', 0.5)}")
                print(f"model_type = '{optimizer.model_type}'")
                print("=" * 40)
                print("è¯´æ˜: è¿™äº›å‚æ•°åœ¨äº¤å‰éªŒè¯ä¸­è¡¨ç°æœ€ä½³")

                # ä¿å­˜å»ºè®®
                with open('recommended_parameters_for_memberB.txt', 'w', encoding='utf-8') as f:
                    f.write("# æˆå‘˜Cæ¨èçš„è¶…å‚æ•°ï¼ˆåŸºäºKæŠ˜äº¤å‰éªŒè¯ï¼‰\n")
                    f.write("# ============================================\n\n")
                    f.write(f"learning_rate = {optimizer.best_params['learning_rate']}\n")
                    f.write(f"batch_size = {optimizer.best_params['batch_size']}\n")
                    f.write(f"optimizer = '{optimizer.best_params['optimizer_type']}'\n")
                    f.write(f"conv_dropout = {optimizer.best_params.get('conv_dropout', 0.25)}\n")
                    f.write(f"fc_dropout = {optimizer.best_params.get('fc_dropout', 0.5)}\n")
                    f.write(f"model_type = '{optimizer.model_type}'\n\n")
                    f.write(f"# äº¤å‰éªŒè¯ç»“æœ:\n")
                    f.write(f"# å¹³å‡å‡†ç¡®ç‡: {optimizer.best_score:.4f} ({optimizer.best_score:.2%})\n")
                    f.write(f"# æ ‡å‡†å·®: {optimizer.best_params.get('std_accuracy', 0):.4f}\n")
                    f.write(f"# å„æŠ˜å‡†ç¡®ç‡: {optimizer.best_params.get('fold_accuracies', [])}\n")

                print("âœ… å»ºè®®å·²ä¿å­˜åˆ° recommended_parameters_for_memberB.txt")
            else:
                print("è¯·å…ˆè¿è¡Œéšæœºæœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°")

        elif choice == '8':
            print("\nğŸ‘‹ é€€å‡ºç¨‹åº")
            print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print("  hyperparameter_tuning_result/ - æ‰€æœ‰ç»“æœæ–‡ä»¶")
            print("  recommended_parameters_for_memberB.txt - ç»™æˆå‘˜Bçš„å»ºè®®")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    try:
        print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
    except:
        print("è¯·å®‰è£…TensorFlow: pip install tensorflow")
        exit(1)

    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists('processed_data'):
        print("âŒ æ‰¾ä¸åˆ° processed_data/ ç›®å½•")
        print("è¯·å…ˆè¿è¡Œ data_preprocessing.py ç”Ÿæˆé¢„å¤„ç†æ•°æ®")
        exit(1)

    # è¿è¡Œä¸»ç¨‹åº
    main()